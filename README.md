# Credit Card Fraud Detection â€” End-to-End Data & ML Pipeline in Databricks
### ğŸ§  Project Overview

This project demonstrates an end-to-end data engineering and machine learning workflow for detecting fraudulent credit card transactions using Databricks Community Edition.

The pipeline simulates a real-world financial data environment, applying Bronzeâ€“Silverâ€“Gold data lake architecture, data quality validation, and a fraud classification model built using Pythonâ€™s scikit-learn.

Unlike traditional Databricks MLlib-based projects, this implementation is fully compatible with Databricks CE, leveraging Spark + Pandas + Delta tables for scalable data prep and persistent model tracking.

#Tech Stack

| Layer                            | Tools / Frameworks                                     |
| -------------------------------- | ------------------------------------------------------ |
| **Data Ingestion (Bronze)**      | Databricks, PySpark, Delta Lake                        |
| **Data Transformation (Silver)** | Spark SQL, PySpark DataFrame APIs                      |
| **Data Quality Validation**      | PySpark (null checks, uniqueness, schema conformance)  |
| **Feature Engineering (Gold)**   | Spark â†’ Pandas                                         |
| **Machine Learning**             | scikit-learn (Logistic Regression, AUC/ROC evaluation) |
| **Model Persistence**            | Delta Table (Base64 serialized model + metadata)       |
| **Orchestration & Versioning**   | Databricks Repos + Git Integration                     |

#Architecture
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Source CSV (Transactions) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Bronze Layer (Raw Data)   â”‚
        â”‚  - Ingest CSV to Delta     â”‚
        â”‚  - Store unprocessed copy  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Silver Layer (Cleansed)   â”‚
        â”‚  - Handle nulls, dups      â”‚
        â”‚  - Validate schema         â”‚
        â”‚  - Apply data quality rulesâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Gold Layer (Features)     â”‚
        â”‚  - Feature scaling & prep  â”‚
        â”‚  - Export to Pandas        â”‚
        â”‚  - Train fraud detection MLâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Model Registry (Delta)    â”‚
        â”‚  - Save model metadata     â”‚
        â”‚  - Serialize model blob    â”‚
        â”‚  - Store metrics (AUC, date)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#Machine Learning Component
After Silver-level cleansing:

Data is converted from Spark to Pandas for local training.
Features are standardized (StandardScaler).
A Logistic Regression model is trained to classify fraudulent vs. legitimate transactions.
Evaluation metrics (AUC, accuracy, recall) are computed and logged.

#ğŸ“ˆ Key Features

- End-to-end pipeline using Databricks Delta Lakehouse pattern
- Automated data quality validation using PySpark
- Feature engineering & ML modeling using scikit-learn
- Model version tracking and storage in Delta
- Git-integrated Databricks workflow for code versioning
- 100% Community Editionâ€“compatible, no DBFS or MLflow dependencies

#Learning Highlights

- How to design a Bronzeâ€“Silverâ€“Gold data lakehouse
- How to integrate Python ML models into a Spark pipeline
- How to build auditability and traceability in CE
- How to leverage Delta tables for persistence beyond DBFS
- How to work effectively with Git + Databricks notebooks